{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AAS SPAM Detector\n",
    "\n",
    "The objective is to develop a SPAM classifier capable of reaching at least 70% accuracy. You can, and should, use all that was presented in the theoretical notebooks.\n",
    "\n",
    "The dataset is different from the toy one used in the class, instead the work will be done on the Enron SPAM dataset. The Enron-Spam dataset is a fantastic ressource collected by V. Metsis, I. Androutsopoulos and G. Paliouras and described in their publication [\"Spam Filtering with Naive Bayes - Which Naive Bayes?\"](https://nes.aueb.gr/ipl/nlp/pubs/ceas2006_paper.pdf). The dataset contains a total of 17.171 spam and 16.545 non-spam (\"ham\") e-mail messages (33.716 e-mails total). The original dataset and documentation can be found [here](http://www2.aueb.gr/users/ion/data/enron-spam/readme.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/goncalomf/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/goncalomf/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/goncalomf/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/goncalomf/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/goncalomf/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import nltk\n",
    "import tqdm\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [15, 15]\n",
    "plt.rcParams['figure.dpi'] = 72\n",
    "import seaborn as sns\n",
    "import seaborn.objects as so\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import gensim\n",
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import matthews_corrcoef, accuracy_score, f1_score\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_csv('datasets/enron_spam_data.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.describe of shape: (42_619, 2)\n",
       "┌─────────────────────────────────┬─────────┐\n",
       "│ text                            ┆ is_spam │\n",
       "│ ---                             ┆ ---     │\n",
       "│ str                             ┆ i64     │\n",
       "╞═════════════════════════════════╪═════════╡\n",
       "│ Re: New Sequences Window        ┆ 0       │\n",
       "│                                 ┆         │\n",
       "│     …                           ┆         │\n",
       "│ [zzzzteana] RE: Alexander       ┆ 0       │\n",
       "│                                 ┆         │\n",
       "│ Mar…                            ┆         │\n",
       "│ [zzzzteana] Moscow bomber       ┆ 0       │\n",
       "│                                 ┆         │\n",
       "│ Man…                            ┆         │\n",
       "│ [IRR] Klez: The Virus That  Wo… ┆ 0       │\n",
       "│ Re: [zzzzteana] Nothing like m… ┆ 0       │\n",
       "│ …                               ┆ …       │\n",
       "│ Какой у тебя любимый цвет? Фан… ┆ 1       │\n",
       "│ Что ты сегодня ел, Чудо-печень… ┆ 1       │\n",
       "│ Что ты думаешь о спорте? Секре… ┆ 1       │\n",
       "│ Какой твой любимый вид спорта,… ┆ 1       │\n",
       "│ Ты умеешь петь? Чудо-птица      ┆ 1       │\n",
       "└─────────────────────────────────┴─────────┘>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _nltk_pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return nltk.corpus.wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return nltk.corpus.wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return nltk.corpus.wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return nltk.corpus.wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "\n",
    "def _nltk_pos_lemmatizer(lemmatizer, token, tag):\n",
    "    if tag is None:\n",
    "        return lemmatizer.lemmatize(token)\n",
    "    else:        \n",
    "        return lemmatizer.lemmatize(token, tag)\n",
    "\n",
    "\n",
    "def text_pre_processing(txt, m=2):\n",
    "    if txt is not None:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "        tokens = nltk.word_tokenize(txt)\n",
    "        tokens = [w for w in tokens if w.isalpha()]\n",
    "        tokens = nltk.pos_tag(tokens)\n",
    "        tokens = [(t[0], _nltk_pos_tagger(t[1])) for t in tokens]\n",
    "        tokens = [_nltk_pos_lemmatizer(lemmatizer, w, t).lower() for w,t in tokens]\n",
    "        tokens = [w for w in tokens if len(w) > m]\n",
    "        tokens = [w for w in tokens if w not in stop_words]\n",
    "    else:\n",
    "        tokens = []\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def div_norm(x):\n",
    "   norm_value = np.linalg.norm(x)\n",
    "   if norm_value > 0:\n",
    "       return x * ( 1.0 / norm_value)\n",
    "   else:\n",
    "       return x\n",
    "\n",
    "\n",
    "def word_vector_to_sentence_vector(sentence:list, model):\n",
    "    vectors = []\n",
    "    # for all the tokens in the setence\n",
    "    for token in sentence:\n",
    "        if token in model:\n",
    "            vectors.append(model[token])\n",
    "    # add the EOS token\n",
    "    if '\\n' in model:\n",
    "        vectors.append(model['\\n'])\n",
    "    # normalize all the vectors\n",
    "    vectors = [div_norm(x) for x in vectors]\n",
    "    return np.mean(vectors, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Re: New Sequences Window\\n\\n    Date:        Wed, 21 Aug 2002 10:54:46 -0500\\n    From:        Chris Garrigues <cwg-dated-1030377287.06fa6d@DeepEddy.Com>\\n    Message-ID:  <1029945287.4797.TMDA@deepeddy.vircio.com>\\n\\n\\n  | I can\\'t reproduce this error.\\n\\nFor me it is very repeatable... (like every time, without fail).\\n\\nThis is the debug log of the pick happening ...\\n\\n18:19:03 Pick_It {exec pick +inbox -list -lbrace -lbrace -subject ftp -rbrace -rbrace} {4852-4852 -sequence mercury}\\n18:19:03 exec pick +inbox -list -lbrace -lbrace -subject ftp -rbrace -rbrace 4852-4852 -sequence mercury\\n18:19:04 Ftoc_PickMsgs {{1 hit}}\\n18:19:04 Marking 1 hits\\n18:19:04 tkerror: syntax error in expression \"int ...\\n\\nNote, if I run the pick command by hand ...\\n\\ndelta$ pick +inbox -list -lbrace -lbrace -subject ftp -rbrace -rbrace  4852-4852 -sequence mercury\\n1 hit\\n\\nThat\\'s where the \"1 hit\" comes from (obviously).  The version of nmh I\\'m\\nusing is ...\\n\\ndelta$ pick -version\\npick -- nmh-1.0.4 [compiled on fuchsia.cs.mu.OZ.AU at Sun Mar 17 14:55:56 ICT 2002]\\n\\nAnd the relevant part of my .mh_profile ...\\n\\ndelta$ mhparam pick\\n-seq sel -list\\n\\n\\nSince the pick command works, the sequence (actually, both of them, the\\none that\\'s explicit on the command line, from the search popup, and the\\none that comes from .mh_profile) do get created.\\n\\nkre\\n\\nps: this is still using the version of the code form a day ago, I haven\\'t\\nbeen able to reach the cvs repository today (local routing issue I think).\\n\\n\\n\\n_______________________________________________\\nExmh-workers mailing list\\nExmh-workers@redhat.com\\nhttps://listman.redhat.com/mailman/listinfo/exmh-workers\\n\\n', \"[zzzzteana] RE: Alexander\\n\\nMartin A posted:\\nTassos Papadopoulos, the Greek sculptor behind the plan, judged that the\\n limestone of Mount Kerdylio, 70 miles east of Salonika and not far from the\\n Mount Athos monastic community, was ideal for the patriotic sculpture. \\n \\n As well as Alexander's granite features, 240 ft high and 170 ft wide, a\\n museum, a restored amphitheatre and car park for admiring crowds are\\nplanned\\n---------------------\\nSo is this mountain limestone or granite?\\nIf it's limestone, it'll weather pretty fast.\\n\\n------------------------ Yahoo! Groups Sponsor ---------------------~-->\\n4 DVDs Free +s&p Join Now\\nhttp://us.click.yahoo.com/pt6YBB/NXiEAA/mG3HAA/7gSolB/TM\\n---------------------------------------------------------------------~->\\n\\nTo unsubscribe from this group, send an email to:\\nforteana-unsubscribe@egroups.com\\n\\n \\n\\nYour use of Yahoo! Groups is subject to http://docs.yahoo.com/info/terms/ \\n\\n\\n\\n\"]\n"
     ]
    }
   ],
   "source": [
    "dataset = df.rows()\n",
    "dataset = [(text, label) for (text, label) in dataset]\n",
    "targets = [label for  _ , label in dataset]\n",
    "corpus =  [txt for  txt, _ in dataset]\n",
    "tokens_clean = [text_pre_processing(text) for text in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model = FastText(vector_size=256, window=7, min_count=3, workers=os.cpu_count(), seed=42)\n",
    "text_model.build_vocab(tokens_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42619/42619 [00:41<00:00, 1031.97it/s]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([word_vector_to_sentence_vector(sentence, text_model.wv) for sentence in tqdm.tqdm(tokens_clean)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, targets, stratify=targets, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goncalomf/Desktop/aas-spam-detector-goncalomf20/venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data : 34095\n",
      "Testing Data  : 8524\n",
      "LR  0.84 0.84 0.67\n",
      "KNN 0.88 0.88 0.76\n",
      "NB  0.57 0.55 0.23\n",
      "RFC 0.86 0.85 0.71\n",
      "MLP 0.92 0.92 0.84\n"
     ]
    }
   ],
   "source": [
    "print(f'Training Data : {len(X_train)}')\n",
    "print(f'Testing Data  : {len(X_test)}')\n",
    "\n",
    "# define the list of classifiers\n",
    "clfs = [\n",
    "    ('LR', LogisticRegression(random_state=42, multi_class='auto', max_iter=600)),\n",
    "    ('KNN', KNeighborsClassifier(n_neighbors=1)),\n",
    "    ('NB', GaussianNB()),\n",
    "    ('RFC', RandomForestClassifier(random_state=42)),\n",
    "    ('MLP', MLPClassifier(random_state=42, learning_rate='adaptive', max_iter=1000))\n",
    "]\n",
    "\n",
    "# whenever possible used joblib to speed-up the training\n",
    "with joblib.parallel_backend('loky', n_jobs=-1):\n",
    "    for label, clf in clfs:\n",
    "        # train the model\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # generate predictions\n",
    "        predictions = clf.predict(X_test)\n",
    "\n",
    "        # compute the performance metrics\n",
    "        mcc = matthews_corrcoef(y_test, predictions)\n",
    "        acc = accuracy_score(y_test, predictions)\n",
    "        f1 = f1_score(y_test, predictions, average='weighted')\n",
    "        print(f'{label:3} {acc:.2f} {f1:.2f} {mcc:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
